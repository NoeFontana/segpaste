{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"segpaste","text":"<p>A PyTorch implementation of \"Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation\" (arXiv:2012.07177).</p> <p>This package also provides integration with <code>torchvision</code> ecosystem.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install segpaste\n</code></pre> <p>Or install from source:</p> <pre><code>git clone https://github.com/NoeFontana/segpaste.git\ncd segpaste\npip install -e .\n</code></pre>"},{"location":"#usage","title":"Usage","text":""},{"location":"#torchvision-integration","title":"TorchVision Integration","text":"<p>Convenience types and wrappers are provided to ease integration with <code>torchvision</code> datasets and transforms.</p> <ul> <li><code>PaddingMask</code>: A TVTensor representing a padding mask.</li> <li><code>CocoDetectionV2</code>: A CocoDetection dataset that presents an interface compatible with <code>torchvision.transforms.v2</code> and with support for padding masks.</li> <li><code>SanitizeBoundingBoxes</code>: A small wrapper around <code>torchvision.transforms.v2.SanitizeBoundingBoxes</code> that adds support for <code>PaddingMask</code>.</li> </ul>"},{"location":"#collator","title":"Collator","text":"<p>The recommended interface is through the <code>CopyPasteCollator</code> which can be used in lieu of a standard collate function in a PyTorch <code>DataLoader</code> as long as the batch_size is greater than 1.</p> <pre><code>from segpaste import CopyPasteAugmentation, CopyPasteCollator, CopyPasteConfig\n\nconfig = CopyPasteConfig()\naugmentation = CopyPasteAugmentation(config)\ncollate_fn = CopyPasteCollator(augmentation)\ntorch.utils.data.DataLoader(\n        dataset, # Your dataset here\n        batch_size=batch_size, # Must be &gt; 1\n        collate_fn=collate_fn,\n    )\n</code></pre> <p>Examples of usage can be found in the test suite.</p>"},{"location":"#transform-unstable-api","title":"Transform (Unstable API)","text":"<p>A minimal working example with <code>CopyPasteTransform</code> can be found in the examples.</p>"},{"location":"#further","title":"Further","text":"<p>The public API is exposed in the <code>segpaste</code> namespace. It is subject to breaking changes, without prior notice, until version 1.0.0.</p>"},{"location":"#development","title":"Development","text":"<p>This project uses Ruff for linting/formatting and mypy for type checking.</p> <pre><code># Install development dependencies\npip install -e \".[dev,coco]\"\n\n# Format, lint, and type-check\nruff format . &amp;&amp; ruff check --fix . &amp;&amp; mypy .\n\n# Run tests\npytest\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome. Please open an issue to discuss major changes or submit a pull request.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use this implementation in your research, please consider citing the original paper:</p> <pre><code>@article{ghiasi2020simple,\n  title={Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation},\n  author={Ghiasi, Golnaz and Cui, Yin and Srinivas, Aravind and Qian, Rui and Lin, Tsung-Yi and Cubuk, Ekin D and Le, Quoc V and Zoph, Barret},\n  journal={arXiv preprint arXiv:2012.07177},\n  year={2020}\n}\n</code></pre>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#segpaste.CopyPasteAugmentation","title":"<code>CopyPasteAugmentation</code>","text":"<p>Copy-paste augmentation for instance segmentation and object detection.</p> <p>This implementation follows the approach described in: \"Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation\" https://arxiv.org/abs/2012.07177</p> Source code in <code>src/segpaste/augmentation/copy_paste.py</code> <pre><code>class CopyPasteAugmentation:\n    \"\"\"Copy-paste augmentation for instance segmentation and object detection.\n\n    This implementation follows the approach described in:\n    \"Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation\"\n    https://arxiv.org/abs/2012.07177\n    \"\"\"\n\n    def __init__(self, config: CopyPasteConfig):\n        \"\"\"Initialize copy-paste augmentation.\n\n        Args:\n            config: Configuration for copy-paste augmentation\n        \"\"\"\n        self.config = config\n\n    def transform(\n        self,\n        target_data: DetectionTarget,\n        source_objects: list[DetectionTarget],\n    ) -&gt; DetectionTarget:\n        \"\"\"Apply copy-paste augmentation to target image.\"\"\"\n        if not self._should_apply_augmentation(target_data, source_objects):\n            return target_data\n\n        selected_objects = self._select_objects_to_paste(source_objects)\n        if not selected_objects:\n            return target_data\n\n        return self._apply_copy_paste(target_data, selected_objects)\n\n    def _should_apply_augmentation(\n        self, target_data: DetectionTarget, source_objects: list[DetectionTarget]\n    ) -&gt; bool:\n        \"\"\"Check if augmentation should be applied.\"\"\"\n        return (\n            len(source_objects) &gt; 0\n            and target_data.masks is not None\n            and random.random() &lt;= self.config.paste_probability\n        )\n\n    def _select_objects_to_paste(\n        self, source_objects: list[DetectionTarget]\n    ) -&gt; list[DetectionTarget]:\n        \"\"\"Select random objects to paste.\"\"\"\n        if not source_objects:\n            return []\n\n        num_available = len(source_objects)\n        max_paste = min(self.config.max_paste_objects, num_available)\n        min_paste = min(self.config.min_paste_objects, max_paste)\n\n        num_to_paste = random.randint(min_paste, max_paste)\n        return random.sample(source_objects, num_to_paste)\n\n    def _apply_copy_paste(\n        self, target_data: DetectionTarget, paste_objects: list[DetectionTarget]\n    ) -&gt; DetectionTarget:\n        \"\"\"Apply copy-paste augmentation to target image.\"\"\"\n        if target_data.masks is None:\n            return target_data\n\n        # Paste objects and collect results\n        pasted_results = self._paste_all_objects(\n            target_data.image.clone(),\n            target_data.padding_mask,\n            paste_objects,\n        )\n\n        if not pasted_results:\n            return target_data\n\n        # Update annotations with pasted objects\n        return self._update_annotations(target_data, pasted_results)\n\n    def _paste_all_objects(\n        self,\n        image: torch.Tensor,\n        padding_mask: torch.Tensor | None,\n        paste_objects: list[DetectionTarget],\n    ) -&gt; list[PlacementResult]:\n        \"\"\"Paste all objects onto the image and return successful placements.\"\"\"\n        pasted_results: list[PlacementResult] = []\n        pasted_boxes: list[torch.Tensor] = []\n\n        target_size = (image.shape[1], image.shape[2])  # (H, W)\n        if padding_mask is not None and padding_mask.shape[1:] != image.shape[1:]:\n            raise ValueError(\"Padding mask shape mismatch\")\n\n        for obj in paste_objects:\n            if not self._is_valid_object(obj):\n                continue\n\n            # Process each mask in the object\n            for i in range(obj.masks.shape[0]):\n                placement = self._try_place_single_object(\n                    image, obj, i, target_size, pasted_boxes, padding_mask\n                )\n\n                if placement:\n                    image = placement.image  # Update for next placement\n                    pasted_boxes.append(placement.box.squeeze(0))\n                    pasted_results.append(placement)\n\n        return pasted_results\n\n    def _is_valid_object(self, obj: DetectionTarget) -&gt; bool:\n        \"\"\"Check if object is valid for pasting.\"\"\"\n        box_h, box_w = (\n            obj.boxes[:, 3] - obj.boxes[:, 1],\n            obj.boxes[:, 2] - obj.boxes[:, 0],\n        )\n\n        # Object must not be too thin\n        min_edge = min(box_h.min().item(), box_w.min().item())\n        if min_edge &lt; self.config.min_object_edge:\n            return False\n        # Object pixel count must be sufficient\n        return (\n            not (obj.masks.sum(dim=(1, 2)) &lt; self.config.min_object_area).any().item()\n        )\n\n    def _try_place_single_object(\n        self,\n        image: torch.Tensor,\n        obj: DetectionTarget,\n        obj_idx: int,\n        target_size: tuple[int, int],\n        existing_boxes: list[torch.Tensor],\n        padding_mask: torch.Tensor | None,\n    ) -&gt; PlacementResult | None:\n        \"\"\"Try to place a single object on the image.\"\"\"\n        # Extract object components (keep batch dimension for consistency)\n        obj_mask = obj.masks[obj_idx : obj_idx + 1]\n        obj_box = obj.boxes[obj_idx : obj_idx + 1]\n        obj_label = obj.labels[obj_idx : obj_idx + 1]\n\n        # Crop source image and mask to bounding box region\n        x1, y1, x2, y2 = obj_box[0].int()\n\n        # Ensure coordinates are within image boundaries\n        img_h, img_w = obj.image.shape[1], obj.image.shape[2]\n        x1 = max(0, min(x1, img_w - 1))\n        y1 = max(0, min(y1, img_h - 1))\n        x2 = max(x1 + 1, min(x2, img_w))\n        y2 = max(y1 + 1, min(y2, img_h))\n\n        cropped_source_image = obj.image[:, y1:y2, x1:x2]\n        cropped_mask = obj_mask[:, y1:y2, x1:x2]\n\n        # Try placement\n        result = self._place_object(\n            image,\n            cropped_source_image,\n            cropped_mask,\n            obj_box,\n            obj_label,\n            target_size,\n            existing_boxes,\n            max_attempts=10,\n            padding_mask=padding_mask,\n        )\n\n        return result\n\n    def _update_annotations(\n        self, target_data: DetectionTarget, pasted_results: list[PlacementResult]\n    ) -&gt; DetectionTarget:\n        \"\"\"Update target annotations with pasted objects.\"\"\"\n        # Extract pasted data\n        final_image = pasted_results[-1].image  # Use final image state\n        pasted_masks = torch.stack([r.mask.squeeze(0) for r in pasted_results])\n        pasted_boxes = torch.stack([r.box.squeeze(0) for r in pasted_results])\n        pasted_labels = torch.stack([r.label.squeeze(0) for r in pasted_results])\n\n        # Update existing objects for occlusion\n        updated_masks, updated_boxes, updated_labels = (\n            self._update_and_filter_occluded_objects(\n                target_data.masks, target_data.boxes, target_data.labels, pasted_masks\n            )\n        )\n\n        # Combine all annotations\n        all_masks = torch.cat([updated_masks, pasted_masks], dim=0)\n        all_boxes = torch.cat([updated_boxes, pasted_boxes], dim=0)\n        all_labels = torch.cat([updated_labels, pasted_labels], dim=0)\n\n        return DetectionTarget(\n            image=final_image,\n            masks=all_masks,\n            boxes=all_boxes,\n            labels=all_labels,\n            padding_mask=target_data.padding_mask,\n        )\n\n    def _place_object(\n        self,\n        target_image: torch.Tensor,\n        source_image: torch.Tensor,\n        source_mask: torch.Tensor,\n        source_box: torch.Tensor,\n        source_label: torch.Tensor,\n        target_size: tuple[int, int],\n        existing_boxes: list[torch.Tensor],\n        max_attempts: int,\n        padding_mask: torch.Tensor | None,\n    ) -&gt; PlacementResult | None:\n        \"\"\"Place an object on the target image.\"\"\"\n        obj_h = int(source_box[0, 3] - source_box[0, 1])\n        obj_w = int(source_box[0, 2] - source_box[0, 0])\n\n        # Try to find valid placement position\n        placement_pos = self._find_valid_placement(\n            target_size,\n            (obj_h, obj_w),\n            existing_boxes,\n            padding_mask,\n            max_attempts,\n        )\n        if placement_pos is None:\n            return None\n\n        top, left = placement_pos\n\n        # Create placed objects\n        placed_image = self._blend_object_on_target(\n            target_image, source_image, source_mask, top, left\n        )\n        placed_mask = self._create_placed_mask(source_mask, target_size, top, left)\n        placed_box = self._create_placed_box(source_box, left, top, obj_w, obj_h)\n\n        return PlacementResult(placed_image, placed_mask, placed_box, source_label)\n\n    def _find_valid_placement(\n        self,\n        target_size: tuple[int, int],\n        object_size: tuple[int, int],\n        existing_boxes: list[torch.Tensor],\n        padding_mask: torch.Tensor | None,\n        max_attempts: int,\n    ) -&gt; tuple[int, int] | None:\n        \"\"\"Find a valid placement position for the object.\"\"\"\n        target_h, target_w = target_size\n        obj_h, obj_w = object_size\n\n        placer = create_object_placer(\n            image_height=target_h,\n            image_width=target_w,\n            existing_boxes=existing_boxes,\n            padding_mask=padding_mask,\n            margin=0,\n            collision_threshold=0.9,\n        )\n\n        candidate = placer.find_valid_placement(obj_h, obj_w, max_attempts)\n\n        if candidate is None:\n            return None\n\n        return candidate.top, candidate.left\n\n    def _create_placed_mask(\n        self,\n        source_mask: torch.Tensor,\n        target_size: tuple[int, int],\n        top: int,\n        left: int,\n    ) -&gt; torch.Tensor:\n        \"\"\"Create mask for placed object in target coordinates.\"\"\"\n        target_h, target_w = target_size\n        source_h, source_w = source_mask.shape[1], source_mask.shape[2]\n\n        placed_mask = torch.zeros(\n            1, target_h, target_w, dtype=source_mask.dtype, device=source_mask.device\n        )\n\n        # Ensure we don't go beyond target boundaries\n        actual_h = min(source_h, target_h - top)\n        actual_w = min(source_w, target_w - left)\n\n        if actual_h &gt; 0 and actual_w &gt; 0:\n            placed_mask[0, top : top + actual_h, left : left + actual_w] = source_mask[\n                0, :actual_h, :actual_w\n            ]\n\n        return placed_mask\n\n    def _create_placed_box(\n        self, source_box: torch.Tensor, left: int, top: int, width: int, height: int\n    ) -&gt; torch.Tensor:\n        \"\"\"Create bounding box for placed object.\"\"\"\n        return torch.tensor(\n            [[left, top, left + width, top + height]],\n            dtype=source_box.dtype,\n            device=source_box.device,\n        )\n\n    def _blend_object_on_target(\n        self,\n        target_image: torch.Tensor,\n        source_image: torch.Tensor,\n        source_mask: torch.Tensor,\n        top: int,\n        left: int,\n    ) -&gt; torch.Tensor:\n        \"\"\"Blend source object onto target image using simple alpha blending.\"\"\"\n        result_image = target_image.clone()\n        source_h, source_w = source_image.shape[1], source_image.shape[2]\n\n        # Extract target region - make sure it doesn't go beyond image bounds\n        target_h, target_w = target_image.shape[1], target_image.shape[2]\n\n        # Clamp the region to fit within target image\n        actual_h = min(source_h, target_h - top)\n        actual_w = min(source_w, target_w - left)\n\n        if actual_h &lt;= 0 or actual_w &lt;= 0:\n            return result_image  # Nothing to blend\n\n        target_region = target_image[:, top : top + actual_h, left : left + actual_w]\n\n        # Crop source image and mask to match the actual region size\n        cropped_source = source_image[:, :actual_h, :actual_w]\n        cropped_mask = source_mask[:, :actual_h, :actual_w]\n\n        # Get the mask and ensure it matches the source image dimensions\n        mask = cropped_mask[0]  # Remove the first dimension\n\n        # Ensure mask has proper dimensions for broadcasting\n        if mask.dim() == 2:\n            mask = mask.unsqueeze(0)  # Add channel dimension\n\n        # Simple alpha blending: target * (1 - mask) + source * mask\n        alpha = 1.0\n        blended_region = target_region * (1.0 - alpha * mask) + cropped_source * (\n            alpha * mask\n        )\n\n        result_image[:, top : top + actual_h, left : left + actual_w] = blended_region\n\n        return result_image\n\n    def _update_and_filter_occluded_objects(\n        self,\n        original_masks: torch.Tensor,\n        original_boxes: torch.Tensor,\n        original_labels: torch.Tensor,\n        pasted_masks: torch.Tensor,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Update masks for occlusion and filter heavily occluded objects.\"\"\"\n        if original_masks.numel() == 0:\n            return original_masks, original_boxes, original_labels\n\n        # Remove occluded parts from original masks\n        occlusion_mask = pasted_masks.sum(dim=0) &gt; 0\n        updated_masks = original_masks * (~occlusion_mask).float()\n\n        # Filter objects based on remaining area\n        valid_indices = self._find_valid_objects(original_masks, updated_masks)\n\n        if valid_indices.numel() == 0:\n            spatial_shape = (original_masks.shape[1], original_masks.shape[2])\n            return self._create_empty_annotations(\n                spatial_shape,\n                original_masks.device,\n                original_masks.dtype,\n                original_boxes.dtype,\n                original_labels.dtype,\n            )\n\n        # Keep only valid objects and recompute boxes\n        filtered_masks = updated_masks[valid_indices]\n        filtered_labels = original_labels[valid_indices]\n        filtered_boxes = masks_to_boxes(filtered_masks)\n\n        return filtered_masks, filtered_boxes, filtered_labels\n\n    def _find_valid_objects(\n        self, original_masks: torch.Tensor, updated_masks: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Find objects that are not too heavily occluded.\"\"\"\n        original_areas = compute_mask_area(original_masks)\n        updated_areas = compute_mask_area(updated_masks)\n\n        # Objects must have remaining area\n        has_area = updated_areas &gt; 0\n\n        # Objects must not be too occluded\n        occlusion_ratios = 1.0 - (updated_areas / (original_areas + 1e-8))\n        not_too_occluded = occlusion_ratios &lt;= self.config.occluded_area_threshold\n\n        return torch.where(has_area &amp; not_too_occluded)[0]\n\n    def _create_empty_annotations(\n        self,\n        spatial_shape: tuple[int, int],\n        device: torch.device,\n        mask_dtype: torch.dtype,\n        box_dtype: torch.dtype,\n        label_dtype: torch.dtype,\n    ) -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n        \"\"\"Create empty annotations when no objects remain.\"\"\"\n        img_height, img_width = spatial_shape\n        empty_masks = torch.empty(\n            (0, img_height, img_width), dtype=mask_dtype, device=device\n        )\n        empty_boxes = torch.empty((0, 4), dtype=box_dtype, device=device)\n        empty_labels = torch.empty((0,), dtype=label_dtype, device=device)\n        return empty_masks, empty_boxes, empty_labels\n</code></pre>"},{"location":"api/#segpaste.CopyPasteAugmentation.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize copy-paste augmentation.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>CopyPasteConfig</code> <p>Configuration for copy-paste augmentation</p> required Source code in <code>src/segpaste/augmentation/copy_paste.py</code> <pre><code>def __init__(self, config: CopyPasteConfig):\n    \"\"\"Initialize copy-paste augmentation.\n\n    Args:\n        config: Configuration for copy-paste augmentation\n    \"\"\"\n    self.config = config\n</code></pre>"},{"location":"api/#segpaste.CopyPasteAugmentation.transform","title":"<code>transform(target_data, source_objects)</code>","text":"<p>Apply copy-paste augmentation to target image.</p> Source code in <code>src/segpaste/augmentation/copy_paste.py</code> <pre><code>def transform(\n    self,\n    target_data: DetectionTarget,\n    source_objects: list[DetectionTarget],\n) -&gt; DetectionTarget:\n    \"\"\"Apply copy-paste augmentation to target image.\"\"\"\n    if not self._should_apply_augmentation(target_data, source_objects):\n        return target_data\n\n    selected_objects = self._select_objects_to_paste(source_objects)\n    if not selected_objects:\n        return target_data\n\n    return self._apply_copy_paste(target_data, selected_objects)\n</code></pre>"},{"location":"api/#segpaste.CopyPasteCollator","title":"<code>CopyPasteCollator</code>","text":"<p>Collate function for batching data with copy-paste augmentation.</p> <p>This collator applies copy-paste augmentation at batch time, allowing objects from different images in the batch to be used as source objects for pasting.</p> Source code in <code>src/segpaste/augmentation/torchvision.py</code> <pre><code>class CopyPasteCollator:\n    \"\"\"Collate function for batching data with copy-paste augmentation.\n\n    This collator applies copy-paste augmentation at batch time,\n    allowing objects from different images in the batch to be used\n    as source objects for pasting.\n    \"\"\"\n\n    def __init__(self, augmentation: CopyPasteAugmentation) -&gt; None:\n        \"\"\"Initialize copy-paste collator.\n\n        Args:\n            augmentation: Copy-paste augmentation instance\n        \"\"\"\n        self.copy_paste: CopyPasteAugmentation = augmentation\n\n    def __call__(\n        self, batch: list[tuple[tv_tensors.Image, dict[str, Any]]]\n    ) -&gt; dict[str, torch.Tensor | list[torch.Tensor]]:\n        \"\"\"Collate batch with copy-paste augmentation.\n\n        Args:\n            batch: List of sample dictionaries\n\n        Returns:\n            Collated batch dictionary.\n            Keys: \"images\", \"boxes\", \"labels\", \"masks\" and optionally \"padding_mask\".\n        \"\"\"\n        if not batch:\n            return {}\n\n        # Extract all objects from batch as potential source objects\n        all_objects: list[DetectionTarget] = []\n        for image, target in batch:\n            target[\"image\"] = image\n\n            # TODO: Revisit. It's wasteful to create separate DetectionTargets.\n            if \"masks\" in target and target[\"masks\"] is not None:\n                num_objects: int = target[\"masks\"].shape[0]\n                for i in range(num_objects):\n                    obj: DetectionTarget = DetectionTarget(\n                        image=target[\"image\"],\n                        boxes=target[\"boxes\"][i : i + 1],\n                        labels=target[\"labels\"][i : i + 1],\n                        masks=target[\"masks\"][i : i + 1],\n                        padding_mask=target.get(\"padding_mask\"),\n                    )\n                    all_objects.append(obj)\n\n        # Apply copy-paste to each sample\n        augmented_samples: list[dict[str, Any]] = []\n        for _, target in batch:\n            target_data: DetectionTarget = DetectionTarget.from_dict(target)\n\n            # Use other objects in batch as source objects\n            source_objects: list[DetectionTarget] = [\n                obj\n                for obj in all_objects\n                if not torch.equal(obj.image, target_data.image)\n            ]\n\n            if source_objects:\n                augmented: DetectionTarget = self.copy_paste.transform(\n                    target_data, source_objects\n                )\n                augmented_sample: dict[str, DetectionTarget.TYPES] = augmented.to_dict()\n                # Copy additional keys\n                for key, value in target.items():\n                    if key not in augmented_sample:\n                        augmented_sample[key] = value\n\n                augmented_samples.append(augmented_sample)\n            else:\n                augmented_samples.append(target)\n\n        # Standard collation\n        return self._collate_samples(augmented_samples)\n\n    def _collate_samples(\n        self, samples: list[dict[str, Any]]\n    ) -&gt; dict[str, torch.Tensor | list[torch.Tensor]]:\n        \"\"\"Collate list of samples into batch tensors.\n\n        Args:\n            samples: List of sample dictionaries\n\n        Returns:\n            Collated batch dictionary\n        \"\"\"\n        if not samples:\n            return {}\n\n        batch: dict[str, torch.Tensor | list[torch.Tensor]] = {}\n\n        batch[\"images\"] = torch.stack([sample[\"image\"] for sample in samples])\n\n        batch[\"boxes\"] = [sample[\"boxes\"] for sample in samples]\n        batch[\"labels\"] = [sample[\"labels\"] for sample in samples]\n        batch[\"masks\"] = [sample[\"masks\"] for sample in samples]\n\n        if samples[0].get(\"padding_mask\") is not None:\n            batch[\"padding_mask\"] = torch.stack(\n                [sample[\"padding_mask\"] for sample in samples]\n            )\n\n        return batch\n</code></pre>"},{"location":"api/#segpaste.CopyPasteCollator.__call__","title":"<code>__call__(batch)</code>","text":"<p>Collate batch with copy-paste augmentation.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>list[tuple[Image, dict[str, Any]]]</code> <p>List of sample dictionaries</p> required <p>Returns:</p> Name Type Description <code>dict[str, Tensor | list[Tensor]]</code> <p>Collated batch dictionary.</p> <code>Keys</code> <code>dict[str, Tensor | list[Tensor]]</code> <p>\"images\", \"boxes\", \"labels\", \"masks\" and optionally \"padding_mask\".</p> Source code in <code>src/segpaste/augmentation/torchvision.py</code> <pre><code>def __call__(\n    self, batch: list[tuple[tv_tensors.Image, dict[str, Any]]]\n) -&gt; dict[str, torch.Tensor | list[torch.Tensor]]:\n    \"\"\"Collate batch with copy-paste augmentation.\n\n    Args:\n        batch: List of sample dictionaries\n\n    Returns:\n        Collated batch dictionary.\n        Keys: \"images\", \"boxes\", \"labels\", \"masks\" and optionally \"padding_mask\".\n    \"\"\"\n    if not batch:\n        return {}\n\n    # Extract all objects from batch as potential source objects\n    all_objects: list[DetectionTarget] = []\n    for image, target in batch:\n        target[\"image\"] = image\n\n        # TODO: Revisit. It's wasteful to create separate DetectionTargets.\n        if \"masks\" in target and target[\"masks\"] is not None:\n            num_objects: int = target[\"masks\"].shape[0]\n            for i in range(num_objects):\n                obj: DetectionTarget = DetectionTarget(\n                    image=target[\"image\"],\n                    boxes=target[\"boxes\"][i : i + 1],\n                    labels=target[\"labels\"][i : i + 1],\n                    masks=target[\"masks\"][i : i + 1],\n                    padding_mask=target.get(\"padding_mask\"),\n                )\n                all_objects.append(obj)\n\n    # Apply copy-paste to each sample\n    augmented_samples: list[dict[str, Any]] = []\n    for _, target in batch:\n        target_data: DetectionTarget = DetectionTarget.from_dict(target)\n\n        # Use other objects in batch as source objects\n        source_objects: list[DetectionTarget] = [\n            obj\n            for obj in all_objects\n            if not torch.equal(obj.image, target_data.image)\n        ]\n\n        if source_objects:\n            augmented: DetectionTarget = self.copy_paste.transform(\n                target_data, source_objects\n            )\n            augmented_sample: dict[str, DetectionTarget.TYPES] = augmented.to_dict()\n            # Copy additional keys\n            for key, value in target.items():\n                if key not in augmented_sample:\n                    augmented_sample[key] = value\n\n            augmented_samples.append(augmented_sample)\n        else:\n            augmented_samples.append(target)\n\n    # Standard collation\n    return self._collate_samples(augmented_samples)\n</code></pre>"},{"location":"api/#segpaste.CopyPasteCollator.__init__","title":"<code>__init__(augmentation)</code>","text":"<p>Initialize copy-paste collator.</p> <p>Parameters:</p> Name Type Description Default <code>augmentation</code> <code>CopyPasteAugmentation</code> <p>Copy-paste augmentation instance</p> required Source code in <code>src/segpaste/augmentation/torchvision.py</code> <pre><code>def __init__(self, augmentation: CopyPasteAugmentation) -&gt; None:\n    \"\"\"Initialize copy-paste collator.\n\n    Args:\n        augmentation: Copy-paste augmentation instance\n    \"\"\"\n    self.copy_paste: CopyPasteAugmentation = augmentation\n</code></pre>"},{"location":"api/#segpaste.CopyPasteConfig","title":"<code>CopyPasteConfig</code>  <code>dataclass</code>","text":"<p>Configuration for copy-paste augmentation.</p> Source code in <code>src/segpaste/config.py</code> <pre><code>@dataclass(frozen=True, slots=True)\nclass CopyPasteConfig:\n    \"\"\"Configuration for copy-paste augmentation.\"\"\"\n\n    # Probability of applying copy-paste augmentation\n    paste_probability: float = 0.5\n\n    # Minimum and maximum number of objects to paste from source to target\n    min_paste_objects: int = 1\n    max_paste_objects: int = 5\n\n    scale_range: tuple[float, float] = (0.5, 2.0)\n    # Blending mode for pasted objects\n    blend_mode: Literal[\"alpha\", \"gaussian\"] = \"alpha\"\n\n    # Min edge length of pasted objects after scaling\n    # Bounding boxes with smaller edges will be skipped\n    min_object_edge: int = 10\n\n    # Min mask area of pasted objects after scaling\n    # Masks with smaller area will be skipped\n    min_object_area: int = 50\n\n    # After pasting, objects with an occlusion ratio above this threshold are removed\n    # Occluded area ratio = 1 - (visible area / original area)\n    # Set to 1.0 to disable this filtering\n    occluded_area_threshold: float = 0.99\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Validate configuration parameters.\"\"\"\n        if not 0.0 &lt;= self.paste_probability &lt;= 1.0:\n            raise ValueError(\"paste_probability must be between 0.0 and 1.0\")\n        if self.min_paste_objects &gt; self.max_paste_objects:\n            raise ValueError(\"min_paste_objects must be &lt;= max_paste_objects\")\n        if self.min_paste_objects &lt; 0:\n            raise ValueError(\"min_paste_objects must be &gt;= 0\")\n        if self.scale_range[0] &gt; self.scale_range[1]:\n            raise ValueError(\"scale_range min must be &lt;= max\")\n        if self.blend_mode not in (\"alpha\", \"gaussian\", \"poisson\"):\n            raise ValueError(\"blend_mode must be one of: alpha, gaussian, poisson\")\n</code></pre>"},{"location":"api/#segpaste.CopyPasteConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate configuration parameters.</p> Source code in <code>src/segpaste/config.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Validate configuration parameters.\"\"\"\n    if not 0.0 &lt;= self.paste_probability &lt;= 1.0:\n        raise ValueError(\"paste_probability must be between 0.0 and 1.0\")\n    if self.min_paste_objects &gt; self.max_paste_objects:\n        raise ValueError(\"min_paste_objects must be &lt;= max_paste_objects\")\n    if self.min_paste_objects &lt; 0:\n        raise ValueError(\"min_paste_objects must be &gt;= 0\")\n    if self.scale_range[0] &gt; self.scale_range[1]:\n        raise ValueError(\"scale_range min must be &lt;= max\")\n    if self.blend_mode not in (\"alpha\", \"gaussian\", \"poisson\"):\n        raise ValueError(\"blend_mode must be one of: alpha, gaussian, poisson\")\n</code></pre>"},{"location":"api/#segpaste.CopyPasteTransform","title":"<code>CopyPasteTransform</code>","text":"<p>               Bases: <code>Module</code></p> <p>PyTorch Transform wrapper for copy-paste augmentation.</p> <p>This transform can be used in torchvision transform pipelines. It expects input data in a dictionary format with keys: - 'image': torch.Tensor of shape [C, H, W] - 'boxes': torch.Tensor of shape [N, 4] in xyxy format - 'labels': torch.Tensor of shape [N] - 'masks': torch.Tensor of shape [N, H, W]</p> Source code in <code>src/segpaste/augmentation/torchvision.py</code> <pre><code>class CopyPasteTransform(torch.nn.Module):\n    \"\"\"PyTorch Transform wrapper for copy-paste augmentation.\n\n    This transform can be used in torchvision transform pipelines.\n    It expects input data in a dictionary format with keys:\n    - 'image': torch.Tensor of shape [C, H, W]\n    - 'boxes': torch.Tensor of shape [N, 4] in xyxy format\n    - 'labels': torch.Tensor of shape [N]\n    - 'masks': torch.Tensor of shape [N, H, W]\n    \"\"\"\n\n    def __init__(\n        self,\n        source_objects: list[DetectionTarget],\n        augmentation: CopyPasteAugmentation,\n    ) -&gt; None:\n        \"\"\"Initialize copy-paste transform.\n\n        Args:\n            source_objects: List of objects that can be pasted\n            augmentation: Copy-paste augmentation instance\n        \"\"\"\n        super().__init__()\n        self.copy_paste: CopyPasteAugmentation = augmentation\n        self.source_objects: list[DetectionTarget] = source_objects\n\n    def forward(self, sample: dict[str, Any]) -&gt; dict[str, DetectionTarget.TYPES]:\n        \"\"\"Apply copy-paste augmentation to sample.\n\n        Args:\n            sample: Dictionary containing image and annotations\n\n        Returns:\n            Augmented sample dictionary\n        \"\"\"\n\n        # Apply copy-paste\n        augmented = self.copy_paste.transform(\n            DetectionTarget.from_dict(sample), self.source_objects\n        )\n\n        # Convert back to dictionary\n        result = augmented.to_dict()\n\n        # Copy any additional keys from original sample\n        for key, value in sample.items():\n            if key not in result:\n                result[key] = value\n\n        return result\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return string representation of transform.\"\"\"\n        return (\n            f\"{self.__class__.__name__}(\"\n            f\"num_source_objects={len(self.source_objects)}, \"\n            f\"config={self.copy_paste.config})\"\n        )\n</code></pre>"},{"location":"api/#segpaste.CopyPasteTransform.__init__","title":"<code>__init__(source_objects, augmentation)</code>","text":"<p>Initialize copy-paste transform.</p> <p>Parameters:</p> Name Type Description Default <code>source_objects</code> <code>list[DetectionTarget]</code> <p>List of objects that can be pasted</p> required <code>augmentation</code> <code>CopyPasteAugmentation</code> <p>Copy-paste augmentation instance</p> required Source code in <code>src/segpaste/augmentation/torchvision.py</code> <pre><code>def __init__(\n    self,\n    source_objects: list[DetectionTarget],\n    augmentation: CopyPasteAugmentation,\n) -&gt; None:\n    \"\"\"Initialize copy-paste transform.\n\n    Args:\n        source_objects: List of objects that can be pasted\n        augmentation: Copy-paste augmentation instance\n    \"\"\"\n    super().__init__()\n    self.copy_paste: CopyPasteAugmentation = augmentation\n    self.source_objects: list[DetectionTarget] = source_objects\n</code></pre>"},{"location":"api/#segpaste.CopyPasteTransform.__repr__","title":"<code>__repr__()</code>","text":"<p>Return string representation of transform.</p> Source code in <code>src/segpaste/augmentation/torchvision.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return string representation of transform.\"\"\"\n    return (\n        f\"{self.__class__.__name__}(\"\n        f\"num_source_objects={len(self.source_objects)}, \"\n        f\"config={self.copy_paste.config})\"\n    )\n</code></pre>"},{"location":"api/#segpaste.CopyPasteTransform.forward","title":"<code>forward(sample)</code>","text":"<p>Apply copy-paste augmentation to sample.</p> <p>Parameters:</p> Name Type Description Default <code>sample</code> <code>dict[str, Any]</code> <p>Dictionary containing image and annotations</p> required <p>Returns:</p> Type Description <code>dict[str, TYPES]</code> <p>Augmented sample dictionary</p> Source code in <code>src/segpaste/augmentation/torchvision.py</code> <pre><code>def forward(self, sample: dict[str, Any]) -&gt; dict[str, DetectionTarget.TYPES]:\n    \"\"\"Apply copy-paste augmentation to sample.\n\n    Args:\n        sample: Dictionary containing image and annotations\n\n    Returns:\n        Augmented sample dictionary\n    \"\"\"\n\n    # Apply copy-paste\n    augmented = self.copy_paste.transform(\n        DetectionTarget.from_dict(sample), self.source_objects\n    )\n\n    # Convert back to dictionary\n    result = augmented.to_dict()\n\n    # Copy any additional keys from original sample\n    for key, value in sample.items():\n        if key not in result:\n            result[key] = value\n\n    return result\n</code></pre>"},{"location":"api/#segpaste.DetectionTarget","title":"<code>DetectionTarget</code>  <code>dataclass</code>","text":"<p>Detection target containing image and annotations.</p> Source code in <code>src/segpaste/types/data_structures.py</code> <pre><code>@dataclass(slots=True)  # Not frozen since it may be modified during augmentation\nclass DetectionTarget:\n    \"\"\"Detection target containing image and annotations.\"\"\"\n\n    image: ImageTensor  # Shape: [C, H, W]\n    boxes: BoxesTensor  # Shape: [N, 4], format: xyxy (top, left, bottom, right)\n    labels: LabelsTensor  # Shape: [N]\n    masks: MasksTensor  # Shape: [N, H, W]\n    padding_mask: PaddingMask | None = None  # Shape: [1, H, W]\n\n    TYPES = ImageTensor | BoxesTensor | LabelsTensor | MasksTensor | PaddingMask | None\n\n    @skip_if_compiling\n    def __post_init__(self) -&gt; None:\n        \"\"\"Validate tensor shapes and consistency.\"\"\"\n        if self.boxes.size(0) != self.labels.size(0):\n            raise ValueError(\"boxes and labels must have same number of objects\")\n        if self.masks.size(0) != self.boxes.size(0):\n            raise ValueError(\"masks and boxes must have same number of objects\")\n        if (\n            self.padding_mask is not None\n            and self.padding_mask.shape[1:] != self.image.shape[1:]\n        ):\n            raise ValueError(\"padding_mask must have same height and width as image\")\n\n    def to_dict(\n        self,\n    ) -&gt; dict[str, TYPES]:\n        \"\"\"Convert DetectionTarget to a dictionary.\"\"\"\n        return {\n            \"image\": self.image,\n            \"boxes\": self.boxes,\n            \"labels\": self.labels,\n            \"masks\": self.masks,\n            \"padding_mask\": self.padding_mask,\n        }\n\n    @staticmethod\n    def from_dict(\n        data: dict[str, Any],\n    ) -&gt; \"DetectionTarget\":\n        \"\"\"Create DetectionTarget from a dictionary.\"\"\"\n        return DetectionTarget(\n            image=data[\"image\"],\n            boxes=data[\"boxes\"],\n            labels=data[\"labels\"],\n            masks=data[\"masks\"],\n            padding_mask=data.get(\"padding_mask\"),\n        )\n</code></pre>"},{"location":"api/#segpaste.DetectionTarget.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate tensor shapes and consistency.</p> Source code in <code>src/segpaste/types/data_structures.py</code> <pre><code>@skip_if_compiling\ndef __post_init__(self) -&gt; None:\n    \"\"\"Validate tensor shapes and consistency.\"\"\"\n    if self.boxes.size(0) != self.labels.size(0):\n        raise ValueError(\"boxes and labels must have same number of objects\")\n    if self.masks.size(0) != self.boxes.size(0):\n        raise ValueError(\"masks and boxes must have same number of objects\")\n    if (\n        self.padding_mask is not None\n        and self.padding_mask.shape[1:] != self.image.shape[1:]\n    ):\n        raise ValueError(\"padding_mask must have same height and width as image\")\n</code></pre>"},{"location":"api/#segpaste.DetectionTarget.from_dict","title":"<code>from_dict(data)</code>  <code>staticmethod</code>","text":"<p>Create DetectionTarget from a dictionary.</p> Source code in <code>src/segpaste/types/data_structures.py</code> <pre><code>@staticmethod\ndef from_dict(\n    data: dict[str, Any],\n) -&gt; \"DetectionTarget\":\n    \"\"\"Create DetectionTarget from a dictionary.\"\"\"\n    return DetectionTarget(\n        image=data[\"image\"],\n        boxes=data[\"boxes\"],\n        labels=data[\"labels\"],\n        masks=data[\"masks\"],\n        padding_mask=data.get(\"padding_mask\"),\n    )\n</code></pre>"},{"location":"api/#segpaste.DetectionTarget.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert DetectionTarget to a dictionary.</p> Source code in <code>src/segpaste/types/data_structures.py</code> <pre><code>def to_dict(\n    self,\n) -&gt; dict[str, TYPES]:\n    \"\"\"Convert DetectionTarget to a dictionary.\"\"\"\n    return {\n        \"image\": self.image,\n        \"boxes\": self.boxes,\n        \"labels\": self.labels,\n        \"masks\": self.masks,\n        \"padding_mask\": self.padding_mask,\n    }\n</code></pre>"},{"location":"api/#segpaste.FixedSizeCrop","title":"<code>FixedSizeCrop</code>","text":"<p>               Bases: <code>Transform</code></p> Source code in <code>src/segpaste/augmentation/lsj.py</code> <pre><code>class FixedSizeCrop(Transform):  # type: ignore[misc]\n    def __init__(\n        self,\n        output_height: int,\n        output_width: int,\n        img_pad_value: float | int = 0,\n        seg_pad_value: int = 255,\n    ) -&gt; None:\n        \"\"\"Crops the given image to a fixed size.\n\n        Args:\n            output_height (int): Desired output height.\n            output_width (int): Desired output width.\n        \"\"\"\n        super().__init__()\n        self.output_height = output_height\n        self.output_width = output_width\n\n        self.img_pad_value = img_pad_value\n        self.seg_pad_value = seg_pad_value\n\n    def make_params(\n        self, flat_inputs: list[tv_tensors.TVTensor | torch.Tensor]\n    ) -&gt; dict[str, int]:\n        inpt_h, inpt_w = query_size(flat_inputs)\n\n        offset_top = round(random.randint(0, max(0, inpt_h - self.output_height)))\n        offset_left = round(random.randint(0, max(0, inpt_w - self.output_width)))\n\n        return {\"offset_top\": offset_top, \"offset_left\": offset_left}\n\n    def transform(\n        self, inpt: tv_tensors.TVTensor | torch.Tensor, params: dict[str, int]\n    ) -&gt; Any:\n        h, w = F.get_size(inpt)\n        cropped = self._call_kernel(\n            F.crop,\n            inpt,\n            top=params[\"offset_top\"],\n            left=params[\"offset_left\"],\n            height=self.output_height,\n            width=self.output_width,\n        )\n        # If the input is smaller than the output size, we pad it\n        if h &lt; self.output_height:\n            if isinstance(cropped, (tv_tensors.Image, tv_tensors.Video)):\n                cropped[..., h:, :] = self.img_pad_value\n            elif isinstance(cropped, tv_tensors.Mask):\n                cropped[..., h:, :] = self.seg_pad_value\n        if w &lt; self.output_width:\n            if isinstance(cropped, (tv_tensors.Image, tv_tensors.Video)):\n                cropped[..., :, w:] = self.img_pad_value\n            elif isinstance(cropped, tv_tensors.Mask):\n                cropped[..., :, w:] = self.seg_pad_value\n\n        return cropped\n</code></pre>"},{"location":"api/#segpaste.FixedSizeCrop.__init__","title":"<code>__init__(output_height, output_width, img_pad_value=0, seg_pad_value=255)</code>","text":"<p>Crops the given image to a fixed size.</p> <p>Parameters:</p> Name Type Description Default <code>output_height</code> <code>int</code> <p>Desired output height.</p> required <code>output_width</code> <code>int</code> <p>Desired output width.</p> required Source code in <code>src/segpaste/augmentation/lsj.py</code> <pre><code>def __init__(\n    self,\n    output_height: int,\n    output_width: int,\n    img_pad_value: float | int = 0,\n    seg_pad_value: int = 255,\n) -&gt; None:\n    \"\"\"Crops the given image to a fixed size.\n\n    Args:\n        output_height (int): Desired output height.\n        output_width (int): Desired output width.\n    \"\"\"\n    super().__init__()\n    self.output_height = output_height\n    self.output_width = output_width\n\n    self.img_pad_value = img_pad_value\n    self.seg_pad_value = seg_pad_value\n</code></pre>"},{"location":"api/#segpaste.PaddingMask","title":"<code>PaddingMask</code>","text":"<p>               Bases: <code>Mask</code></p> <p>Unlike tv_tensor.Mask, PaddingMask is not associated with any object.</p> <p>It is used to indicate padded parts of an Image. Unlike tv_tensor.Mask, it is is forwarded unchanged by this package reimplementation SanitizeBoundingBoxes.</p> Source code in <code>src/segpaste/types/data_structures.py</code> <pre><code>class PaddingMask(Mask):  # type: ignore[misc]\n    \"\"\"Unlike tv_tensor.Mask, PaddingMask is not associated with any object.\n\n    It is used to indicate padded parts of an Image. Unlike tv_tensor.Mask, it is\n    is forwarded unchanged by this package reimplementation SanitizeBoundingBoxes.\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"api/#segpaste.RandomResize","title":"<code>RandomResize</code>","text":"<p>               Bases: <code>Transform</code></p> Source code in <code>src/segpaste/augmentation/lsj.py</code> <pre><code>class RandomResize(Transform):  # type: ignore[misc]\n    def __init__(\n        self,\n        min_scale: float,\n        max_scale: float,\n        target_height: int,\n        target_width: int,\n    ) -&gt; None:\n        \"\"\"Randomly resize the input image while preserving aspect ratio.\n\n        The final size is obtained by scaling the target height and width with a random\n        factor.\n        \"\"\"\n\n        super().__init__()\n        self.min_scale = min_scale\n        self.max_scale = max_scale\n        self.target_height = target_height\n        self.target_width = target_width\n\n    def make_params(\n        self,\n        flat_inputs: list[tv_tensors.TVTensor | torch.Tensor],  # noqa: ARG002\n    ) -&gt; dict[str, float]:\n        scale = random.uniform(self.min_scale, self.max_scale)\n        return {\"scale\": scale}\n\n    def transform(\n        self, inpt: tv_tensors.TVTensor | torch.Tensor, params: dict[str, float]\n    ) -&gt; Any:\n        h, w = F.get_size(inpt)\n        scale: float = params[\"scale\"]\n\n        target_scale_h, target_scale_w = (\n            self.target_height * scale,\n            self.target_width * scale,\n        )\n        output_scale = min(target_scale_h / h, target_scale_w / w)\n\n        new_h, new_w = round(h * output_scale), round(w * output_scale)\n        return self._call_kernel(F.resize, inpt, [new_h, new_w])\n</code></pre>"},{"location":"api/#segpaste.RandomResize.__init__","title":"<code>__init__(min_scale, max_scale, target_height, target_width)</code>","text":"<p>Randomly resize the input image while preserving aspect ratio.</p> <p>The final size is obtained by scaling the target height and width with a random factor.</p> Source code in <code>src/segpaste/augmentation/lsj.py</code> <pre><code>def __init__(\n    self,\n    min_scale: float,\n    max_scale: float,\n    target_height: int,\n    target_width: int,\n) -&gt; None:\n    \"\"\"Randomly resize the input image while preserving aspect ratio.\n\n    The final size is obtained by scaling the target height and width with a random\n    factor.\n    \"\"\"\n\n    super().__init__()\n    self.min_scale = min_scale\n    self.max_scale = max_scale\n    self.target_height = target_height\n    self.target_width = target_width\n</code></pre>"},{"location":"api/#segpaste.SanitizeBoundingBoxes","title":"<code>SanitizeBoundingBoxes</code>","text":"<p>               Bases: <code>SanitizeBoundingBoxes</code></p> Source code in <code>src/segpaste/augmentation/lsj.py</code> <pre><code>class SanitizeBoundingBoxes(tv_SanitizeBoundingBoxes):  # type: ignore[misc]\n    def transform(self, inpt: Any, params: dict[str, Any]) -&gt; Any:\n        \"\"\"Unlike the original SanitizeBoundingBoxes, this transform can also handle\n        PaddingMask and will forward them unchanged.\"\"\"\n\n        is_label = params[\"labels\"] is not None and any(\n            inpt is label for label in params[\"labels\"]\n        )\n        if is_label:\n            return inpt[params[\"valid\"]]\n\n        is_bounding_boxes_or_mask = isinstance(\n            inpt, (tv_tensors.BoundingBoxes, tv_tensors.Mask)\n        ) and not isinstance(inpt, PaddingMask)\n        if not is_bounding_boxes_or_mask:\n            return inpt\n\n        return tv_tensors.wrap(inpt[params[\"valid\"]], like=inpt)\n</code></pre>"},{"location":"api/#segpaste.SanitizeBoundingBoxes.transform","title":"<code>transform(inpt, params)</code>","text":"<p>Unlike the original SanitizeBoundingBoxes, this transform can also handle PaddingMask and will forward them unchanged.</p> Source code in <code>src/segpaste/augmentation/lsj.py</code> <pre><code>def transform(self, inpt: Any, params: dict[str, Any]) -&gt; Any:\n    \"\"\"Unlike the original SanitizeBoundingBoxes, this transform can also handle\n    PaddingMask and will forward them unchanged.\"\"\"\n\n    is_label = params[\"labels\"] is not None and any(\n        inpt is label for label in params[\"labels\"]\n    )\n    if is_label:\n        return inpt[params[\"valid\"]]\n\n    is_bounding_boxes_or_mask = isinstance(\n        inpt, (tv_tensors.BoundingBoxes, tv_tensors.Mask)\n    ) and not isinstance(inpt, PaddingMask)\n    if not is_bounding_boxes_or_mask:\n        return inpt\n\n    return tv_tensors.wrap(inpt[params[\"valid\"]], like=inpt)\n</code></pre>"},{"location":"api/#segpaste.make_large_scale_jittering","title":"<code>make_large_scale_jittering(output_size, min_scale=0.1, max_scale=2.0, img_pad_value=0, seg_pad_value=255)</code>","text":"<p>Factory function to create a LargeScaleJittering transform.</p> <p>Parameters:</p> Name Type Description Default <code>output_size</code> <code>int or tuple</code> <p>The desired output size (height, width) of the crop.</p> required <code>min_scale</code> <code>float</code> <p>The minimum scale factor for resizing.</p> <code>0.1</code> <code>max_scale</code> <code>float</code> <p>The maximum scale factor for resizing.</p> <code>2.0</code> <code>img_pad_value</code> <code>float or int</code> <p>Fill value for image padding.</p> <code>0</code> <code>seg_pad_value</code> <code>int</code> <p>Fill value for segmentation mask padding.</p> <code>255</code> <p>Returns:</p> Type Description <code>Transform</code> <p>A Compose transform implementing Large Scale Jittering.</p> Source code in <code>src/segpaste/augmentation/lsj.py</code> <pre><code>def make_large_scale_jittering(\n    output_size: int | tuple[int, int],\n    min_scale: float = 0.1,\n    max_scale: float = 2.0,\n    img_pad_value: float | int = 0,\n    seg_pad_value: int = 255,\n) -&gt; Transform:\n    \"\"\"\n    Factory function to create a LargeScaleJittering transform.\n\n    Args:\n        output_size (int or tuple): The desired output size (height, width) of the crop.\n        min_scale (float): The minimum scale factor for resizing.\n        max_scale (float): The maximum scale factor for resizing.\n        img_pad_value (float or int): Fill value for image padding.\n        seg_pad_value (int): Fill value for segmentation mask padding.\n\n    Returns:\n        A Compose transform implementing Large Scale Jittering.\n    \"\"\"\n    if isinstance(output_size, int):\n        output_size = (output_size, output_size)\n\n    output_height, output_width = output_size\n\n    return Compose(\n        [\n            RandomResize(\n                min_scale=min_scale,\n                max_scale=max_scale,\n                target_height=output_height,\n                target_width=output_width,\n            ),\n            FixedSizeCrop(\n                output_height=output_height,\n                output_width=output_width,\n                img_pad_value=img_pad_value,\n                seg_pad_value=seg_pad_value,\n            ),\n        ]\n    )\n</code></pre>"}]}